http://www.cnblogs.com/wwxbi/p/6214908.html



训练集有多大？
	如果你的训练集很小，高偏差/低方差的分类器（如朴素贝叶斯）比低偏差/高方差的分类器（如K近邻或Logistic回归）更有优势，因为后者容易过拟合。
	但是随着训练集的增大，高偏差的分类器并不能训练出非常准确的模型，所以低偏差/高方差的分类器会胜出（它们有更小的渐近误差）。
	你也可以从生成模型与鉴别模型的区别来考虑它们。


某些分类器的优势
	朴素贝叶斯(Naive Bayes, NB)
		超级简单，就像做一些数数的工作。如果条件独立假设成立的话，NB将比鉴别模型（如Logistic回归）收敛的更快，所以你只需要少量的训练数据。
		即使条件独立假设不成立，NB在实际中仍然表现出惊人的好。如果你想做类似半监督学习，或者是既要模型简单又要性能好，NB值得尝试。
	Logistic回归(Logistic Regression, LR)
		LR有很多方法来对模型正则化。比起NB的条件独立性假设，LR不需要考虑样本是否是相关的。
		与决策树与支持向量机（SVM）不同，NB有很好的概率解释，且很容易利用新的训练数据来更新模型（使用在线梯度下降法）。
		如果你想要一些概率信息（如，为了更容易的调整分类阈值，得到分类的不确定性，得到置信区间），或者希望将来有更多数据时能方便的更新改进模型，LR是值得使用的。
	决策树（Decision Tree, DT）
		DT是非参数的，所以你不需要担心野点（或离群点）和数据是否线性可分的问题
		（例如，DT可以轻松的处理这种情况：属于A类的样本的特征x取值往往非常小或者非常大，而属于B类的样本的特征x取值在中间范围）。
		DT的主要缺点是容易过拟合，这也正是随机森林（Random Forest, RF）（或者Boosted树）等集成学习算法被提出来的原因。
		此外，RF在很多分类问题中经常表现得最好（我个人相信一般比SVM稍好），且速度快可扩展，也不像SVM那样需要调整大量的参数，所以最近RF是一个非常流行的算法。
	支持向量机（Support Vector Machine, SVM）
		很高的分类正确率，对过拟合有很好的理论保证，选取合适的核函数，面对特征线性不可分的问题也可以表现得很好。
		SVM在维数通常很高的文本分类中非常的流行。由于较大的内存需求和繁琐的调参，我认为RF已经开始威胁其地位了。

	回到LR与DT的问题（我更倾向是LR与RF的问题），做个简单的总结：
		两种方法都很快且可扩展。
		在正确率方面，RF比LR更优。
		但是LR可以在线更新且提供有用的概率信息。鉴于你在Square(不确定推断科学家是什么，应该不是有趣的化身)，
		可能从事欺诈检测：如果你想快速的调整阈值来改变假阳性率与假阴性率，分类结果中包含概率信息将很有帮助。

		无论你选择什么算法，如果你的各类样本数量是不均衡的（在欺诈检测中经常发生），你需要重新采样各类数据或者调整你的误差度量方法来使各类更均衡。
		但是。。。更好的数据往往比更好的算法更重要，提取好的特征也需要很大的功夫。
		如果你的数据集非常大，那么分类算法的选择可能对最后的分类性能影响并不大（所以可以根据运行速度或者易用性来选择）。
		如果你很在意分类的正确率，那么你得尝试多种分类器，根据交叉验证的结果来挑选性能最好的。
		或者，学习下Netflix Prize和Middle Earth, 使用某种集成的方法来组合多个分类器。

对于一个特定问题而言，我们往往可以使用多种机器学习算法，也就是说，对于一个相同的问题，可以有很多不同的机器学习模型来解决它。
所以挑选出最适合某个问题的机器学习模型就成为了一种艺术，一种需要具备深厚耐心并能够坚持长时间调试错误的艺术。


监督学习算法的主要分类包括：
		分类算法(Classification algorithms)：这种算法使用训练数据（训练数据中包含特征以及分类标签）来构建预测模型。这些预测模型可以使用它从训练数据中所挖掘出来的特征来对新的、未曾见过的数据集进行分类标签预测，而这种最终的分类结果是相互分离的。常用的分类算法有决策树、随机森林、支持向量机等等。
		回归算法(Regression algorithms)：这种算法使用从输入数据中获得的特征参数来预测一些额外的特征，为了完成这一工作，算法会建立一个基于数据特征的模型，这个模型可以针对训练数据给出一些未知的特征，也可以用来预测新数据集的特征属性。这里输出的特征属性是连续的且互不分离的。常见的回归算法包括线性回归、多元回归、回归树和套索回归等等。
		监督学习的具体例子有语音识别、信用评估、医学成像以及搜索引擎等。









正则化的前提是-特征值要进行归一化。
	：在实际应该过程中，为了增强模型的(泛化能力)，防止我们训练的模型过拟合，特别是对于大量的稀疏特征，模型复杂度比较高，需要进行降维，我们需要保证在-(训练误差最小化)的基础上，通过加上正则化项减小模型复杂度。
	：在逻辑回归中，有L1、L2进行正则化。

	：在损失函数里加入一个正则化项，正则化项就是权重的L1或者L2范数乘以一个λ，用来控制损失函数和正则化项的比重，直观的理解，首先防止过拟合的目的就是防止最后训练出来的模型过分的依赖某一个特征，当最小化损失函数的时候，(某一维度很大)，拟合出来的函数值与真实的值之间的差距很小，通过正则化可以使(整体的cost)变大，从而避免了过分依赖某一维度的结果。当然加正则化的前提是特征值要进行归一化。


（1）解决高偏差
    增加更多的特征，让模型更复杂。增大k(最近邻)(逻辑回归是Reg正则参数)，使更多的近邻被考虑进来，或者删减一些特征。
    ：（较小的k/c值会带来较大的惩罚，也就使模型更复杂）

    ：特性（测试误差在开始时有些下降，但之后会维持在一个很高的数值上，同时随着数据集规模的增大，训练误差会与测试误差较为接近）

    ：特性（测试和训练误差很接近，但都处于难以接受的较高数值上--Eroor=0.4。--意味着逻辑回归在目前的特征空间是欠拟合的，无法学到一个能够正确拟合数据的模型）


（2）解决高方差
    意味着模型太复杂了，只能尝试更多的数据，或者降低模型复杂度。
    ：特性（高仿差可以通过-训练和测试误差-两条曲线之间的巨大差距识别出来）



用于分类任务。方法是把一个线性模型拟合成某个类型的概率分布，然后用一个函数建立阈值来确定结果属于哪一类。
