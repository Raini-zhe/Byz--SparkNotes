ML Regression所有算法
    Linear regression
    Generalized linear regression
      Available families
    Decision tree regression
    Random forest regression
    Gradient-boosted tree regression
    Survival regression
    Isotonic regression



监督机器学习问题无非就是在规则化参数的同时最小化误差。
    最小化误差是为了让我们的模型拟合我们的训练数据，而规则化参数是防止我们的模型过分拟合我们的训练数据。


线性回归
    线性拟合，就是预测函数是一条直线，对于眼前一堆分布貌似有规律的点.
    线性回归(Linear Regression)是利用称为线性回归方程的最小平方函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析。这种函数是一个或多个称为回归系数的模型参数的线性组合。

    回归分析中，只包括一个自变量和一个因变量，且二者的关系可用一条直线近似表示，这种回归分析称为一元线性回归分析。
    如果回归分析中包括两个或两个以上的自变量，且因变量和自变量之间是线性关系，则称为多元线性回归分析。

    线性回归假设特征和结果满足线性关系。其实线性关系的表达能力非常强大，每个特征对结果的影响强弱可以有前面的参数体现，而且每个特征变量可以首先映射到一个函数，然后再参与线性计算，这样就可以表达特征与结果之间的非线性关系。


目标求解方法有：
    （1）梯度下降算法
    （2）批量梯度下降算法
    （3）随机梯度下降算法
    （4）最小二乘法



损失函数（也称成本函数）：
    我们假定的这条直线的输出与训练数据的标签的差值的和，再加上一个正则项。

    机器学习的大部分带参模型都和这个不但形似，而且神似。是的，其实大部分无非就是变换这两项而已。
    对于第一项Loss函数，如果是Square loss，那就是最小二乘了；
                     如果是Hinge Loss，那就是著名的SVM了；
                     如果是exp-Loss，那就是牛逼的 Boosting了；
                     如果是log-Loss，那就是Logistic Regression了；等等。不同的loss函数，具有不同的拟合特性，这个也得就具体问题具体分析的。


正则化的目的是鼓励模型简单、避免过拟合。
    根据正则项的不同，在Spark中分出三种回归：（Lasso，Ridge和ElasticNet(LinearRegression) ）在高维和数据集变量之间多重共线性情况下运行良好。

    数学上，ElasticNet被定义为L1和L2正则化项的凸组合
      通过适当设置α，ElasticNet包含L1和L2正则化作为特殊情况。
      采用L1(参数α设置为1)正则化时为Lasso回归（元素绝对值），采用L2(α设置为0)时为RidgeRegression回归（元素平方），没有正则化时就是线性回归。



：L2由于比L1平滑,问题通常更容易解决。然而，L1正规化可以帮助促进稀疏导致更小和更可解释的模型，后者可以是有用的特征选择的权重。弹性网是L1和L2正则化的组合。不建议训练模型没有任何正规化，特别是当训练的例子是小。


  L1范数
    L1范数是指向量中各个元素绝对值之和，也有个美称叫“稀疏规则算子”（Lasso regularization）。L0可以使特征稀疏，但是不容易求解，L1是L0范数的最优凸近似（V型趋进最优值，而不是u型），比L0范数要容易优化求解。
    L1可以实现稀疏：
      1）特征选择(Feature Selection)：它会学习地去掉一些没有信息的无用特征，也就是把这些特征对应的权重置为0。
      2）可解释性(Interpretability)：模型更容易解释。
         例如：患某种病的概率是y，收集的数据x是1000维的，也就是我们需要寻找这1000种因素到底是怎么影响患上这种病的概率的。
              通过学习，如果最后学习到的w*就只有很少的非零元素，例如只有5个非零的wi，那么我们就有理由相信，这些对应的特征在患病分析上面提供的信息是巨大的，决策性的。也就是说，患不患这种病只和这5个因素有关，那医生就好分析多了。

  L2范数
    “岭回归”（Ridge Regression），“权值衰减weight decay” -- 改善：过拟合，还可以让我们的优化求解变得稳定和快速（近似V型趋进求解最优值，而不是u型缓慢的求）
    L2范数是指向量各元素的平方和然后求平方根。
      ：让L2范数的规则项||W||2最小，可以使得W的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0。而越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象。实现了对模型空间的限制，提升模型的泛化能力
    L2范数有助于处理 condition number不好的情况下矩阵求逆很困难的问题。
      ：condition number就是拿来衡量ill-condition系统的可信度的。condition number衡量的是输入发生微小变化的时候，输出会发生多大的变化。也就是系统对微小变化的敏感度。condition number值小的就是well-conditioned的（好的值)，大的就是ill-conditioned（不好的值）。
    L2范数不但可以防止过拟合。


  L1和L2的差别：
    为什么一个让绝对值最小，一个让平方最小，会有那么大的差别呢？我看到的有两种几何上直观的解析：
    1）下降速度：
           我们知道，L1和L2都是规则化的方式，我们将权值参数以L1或者L2的方式放到代价函数里面去。然后模型就会尝试去最小化这些权值参数。
           而这个最小化就像一个下坡的过程，L1和L2的差别就在于这个“坡”不同，
           ：L1就是按绝对值函数的“坡”下降的（V型下降），而L2是按二次函数的“坡”下降（u型下降）。所以L1的下降速度比L2的下降速度要快。所以会非常快得降到最小值0。
    2）模型空间的限制：
        L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。
        ：L1的求解曲线是v型与等高线在每个坐标轴相交的地方都有“角”出现，在角的位置就会产生稀疏性。相比之下，L2-ball 就没有这样的性质，因为没有角，所以第一次相交的地方出现在具有稀疏性的位置的概率就变得非常小了。这就从直观上来解释了为什么L1-regularization 能产生稀疏性，而L2-regularization 不行的原因了。



  总结：
      Lasso在特征选择时候非常有用，而Ridge就只是一种规则化而已。




      机器学习的大部分带参模型都和这个不但形似，而且神似。是的，其实大部分无非就是变换这两项而已。
      对于第一项Loss函数，如果是Square loss，那就是最小二乘了；
                       如果是Hinge Loss，那就是著名的SVM了；
                       如果是exp-Loss，那就是牛逼的 Boosting了；
                       如果是log-Loss，那就是Logistic Regression了；等等。不同的loss函数，具有不同的拟合特性，这个也得就具体问题具体分析的。
