KNN最邻近规则，主要应用领域是对未知事物的识别，即判断未知事物属于哪一类，判断思想是，基于距离计算公式（欧几里得定理，余弦定理（文本分析）），判断未知事物的特征和哪一类已知事物的的特征最接近； 
K最近邻(k-Nearest Neighbor，KNN)分类算法，是一个理论上比较成熟的方法，也是最简单的机器学习算法之一。该方法的思路是：如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。KNN算法中，所选择的邻居都是已经正确分类的对象。该方法在定类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。 KNN方法虽然从原理上也依赖于极限定理，但在类别决策时，只与极少量的相邻样本有关。由于KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，KNN方法较其他方法更为适合。 
　　KNN算法不仅可以用于分类，还可以用于回归。通过找出一个样本的k个最近邻居，将这些邻居的属性的平均值赋给该样本，就可以得到该样本的属性。更有用的方法是将不同距离的邻居对该样本产生的影响给予不同的权值(weight)，如权值与距离成正比（组合函数）。 
　　该算法在分类时有个主要的不足是，当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。 该算法只计算“最近的”邻居样本，某一类的样本数量很大，那么或者这类样本并不接近目标样本，或者这类样本很靠近目标样本。无论怎样，数量并不能影响运行结果。可以采用权值的方法（和该样本距离小的邻居权值大）来改进。该方法的另一个不足之处是计算量较大，因为对每一个待分类的文本都要计算它到全体已知样本的距离，才能求得它的K个最近邻点。目前常用的解决方法是事先对已知样本点进行剪辑，事先去除对分类作用不大的属性。该算法比较适用于样本容量比较大的类域的自动分类，而那些样本容量较小的类域采用这种算法比较容易产生误分。 
简单来说，K-NN可以看成：有那么一堆你已经知道分类的数据，然后当一个新数据进入的时候，就开始跟训练数据里的每个点求距离，然后挑离这个训练数据最近的K个点看看这几个点属于什么类型，然后用少数服从多数的原则，给新数据归类。

类别的判定规则： 
1）、投票决定：少数服从多数，近邻中哪个类别的点最多就分为该类。 
2）、加权投票法：根据距离的远近，对近邻的投票进行加权，距离越近则权重越大（权重为距离平方的倒数）

算法的优缺点： 
优点： 
1）简单、有效。 
2）重新训练的代价较低（类别体系的变化和训练集的变化，在Web环境和电子商务 应用中是很常见的）。 
3）计算时间和空间线性于训练集的规模（在一些场合不算太大）。 
4）由于KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所 属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，KNN方法较其他方法 更为适合。 
5）该算法比较适用于样本容量比较大的类域的自动分类，而那些样本容量较小的类域 采用这种算法比较容易产生误分。 
缺点： 
1）KNN算法是懒散学习方法（lazy learning,基本上不学习），一些积极学习的算法 要 快很多。 
2）类别评分不是规格化的（不像概率评分）。 
3）输出的可解释性不强，例如决策树的可解释性较强。 
4）该算法在分类时有个主要的不足是，当样本不平衡时，如一个类的样本容量很大， 而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大 容量类的样本占多数。该算法只计算“最近的”邻居样本，某一类的样本数量很大，那 么或者这类样本并不接近目标样本，或者这类样本很靠近目标样本。无论怎样，数量并 不能影响运行结果。可以采用权值的方法（和该样本距离小的邻居权值大）来改进。 
5）计算量较大。目前常用的解决方法是事先对已知样本点进行剪辑，事先去除对分类 作用不大的样本。

常见问题：

1、k值设定为多大？ 
k太小，分类结果易受噪声点影响；k太大，近邻中又可能包含太多的其它类别的点。（对距离加权，可以降低k值设定的影响） 
k值通常是采用交叉检验来确定（以k=1为基准） 
经验规则：k一般低于训练样本数的平方根 
2、类别如何判定最合适？ 
投票法没有考虑近邻的距离的远近，距离更近的近邻也许更应该决定最终的分类，所以加权投票法更恰当一些。 
3、高维度对距离衡量的影响：众所周知当变量数越多，欧式距离的区分能力就越差。 
变量值域对距离的影响：值域越大的变量常常会在距离计算中占据主导作用，因此应先对变量进行标准化。 
4、训练样本是否要一视同仁？ 
在训练集中，有些样本可能是更值得依赖的。 
可以给不同的样本施加不同的权重，加强依赖样本的权重，降低不可信赖样本的影响 
5、性能问题？ 
kNN是一种懒惰算法，平时不好好学习，考试（对测试样本分类）时才临阵磨枪（临时去找k个近邻）。 
懒惰的后果：构造模型很简单，但在对测试样本分类地的系统开销大，因为要扫描全部训练样本并计算距离。 
已经有一些方法提高计算的效率，例如压缩训练样本量等。 
6、能否大幅减少训练样本量，同时又保持分类精度？ 
浓缩技术(condensing) 
编辑技术(editing)

算法步骤： 
step.1—初始化距离为最大值maxdist 
step.2—计算未知样本和每个训练样本的距离dist 
step.3—得到目前K个最临近样本中的最大距离maxdist 
step.4—如果dist小于maxdist，则将该训练样本作为K-最近邻样本 
step.5—重复步骤2、3、4，直到未知样本和所有训练样本的距离都算完 
step.6—统计K-最近邻样本中每个类标号出现的次数 
step.7—选择出现频率最大的类标号作为未知样本的类标号

K近邻不仅不仅可以用来分类，也可以用来回归，也可以用来做推荐。算法可以稍加修改，解决不应的应用场景的问题


