
（决策树不需要标准化，也不要求类型特征二元编码）

一/ Feature Transformers（特征变换）/---------------------------------------------/

1.Tokenizer（分词器）--- 默认空格分隔
2.RegexTokenizer --- 参数"pattern"(默认的正则分隔符: "\\s+") 用于拆分输入的文本。.setPattern("\\W")

3.StopWordsRemover（停用字清除）--- 通过调用StopWordsRemover.loadDefaultStopWords(language)设置（不支持中文）


4.n-gram
NGram 输入为一系列的字符串（例如：Tokenizer分词器的输出）。参数n表示每个n个单词合并成一个（并用空格分割这一个单词）。如果输入的字符串少于n个单词，NGram输出为空。


5.Binarizer(二元化方法) 
（输入的）特征值大于阀值将映射为1.0，特征值小于等于阀值将映射为0.0。（Binarizer）支持向量（Vector）和双精度（Double）类型的输出。
符合贝叶斯算法输入。


6.PCA（主成分分析）
主成分分析是一种统计学方法，它使用正交转换从一系列可能线性相关的变量中提取线性无关变量集，提取出的变量集中的元素称为主成分（principal components）。如-将5维特征向量转换为3维主成分向量，PCA().setK(3)


7.PolynomialExpansion（多项式扩展）
将n维的原始特征组合扩展到多项式空间的过程。new PolynomialExpansion().setDegree(3) 将原始向量扩展到3维空间。
	如：Vectors.dense(0.0, 0.0),扩展后：[[ 0.0,0.0,0.0, 0.0,0.0,0.0, 0.0,0.0,0.0 ]] -- 3维


8.Discrete Cosine Transform (DCT-离散余弦变换)
	将（时域）的N维实数序列转换成（频域）的N维实数序列的过程（有点类似离散傅里叶变换）--维度不变。
	将离散余弦变换后结果乘以（1/根号2）得到一个与时域矩阵长度一致的矩阵，new DCT().setInverse(false)， 输入序列与输出之间是一一对应的。


9.StringIndexer（字符串-索引变换）
	类型特征映射为数字特征 -- 将字符串的（以单词为）标签编码成标签索引（表示）。标签索引序列的取值范围是[0，numLabels（字符串中所有出现的单词去掉重复的词后的总和）]，按照标签出现频率排序，出现最多的标签索引为0。如果输入是数值型，我们先将数值映射到字符串，再对字符串进行索引化。
	new StringIndexer().setHandleInvalid("skip") 对没出现过的标签跳过，“error“为抛出异常。


10.IndexToString（索引-字符串变换）
	与StringIndexer对应，IndexToString将索引化标签还原成原始字符串。一个常用的场景是先通过StringIndexer产生索引化标签，然后使用索引化标签进行训练，最后再对预测结果使用IndexToString来获取其原始的标签字符串。


11.OneHotEncoder（独热编码）
	import org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer}
	将类别特征映射为二进制向量，有效值（为1，其余为0）-- 先StringIndexer，再OneHotEncoder。
	（试验出现问题，3个特征只转了两个，还有一个为01之外的数字）


12.VectorIndexer(向量类型索引化)
	VectorIndexer是对数据集特征向量中的类别特征（index categorical features categorical features ，eg：枚举类型）进行编号索引。它能够自动判断那些特征是可以重新编号的类别型，并对他们进行重新编号索引，具体做法如下：
	1.获得一个向量类型的输入以及maxCategories参数。
	2.基于原始向量数值识别哪些特征需要被类别化：特征向量中某一个特征不重复取值个数小于等于maxCategories则认为是可以重新编号索引的。某一个特征不重复取值个数大于maxCategories，则该特征视为连续值，不会重新编号（不会发生任何改变）
	3.对于每一个可编号索引的类别特征重新编号为0～K（K<=maxCategories-1）。
	4.对类别特征原始值用编号后的索引替换掉。
	索引后的类别特征可以帮助决策树等算法处理类别型特征，提高性能。-(例子中有692个特征(列)，K=10时 有351个特征可以被索引化)








//---（ 1-3 这些正则方法只针对稠密特征有效，在稀疏特征上不会给出好的结果。）(只针对回归，决策树+贝叶斯 不受特征标准化影响)


1.Normalizer(范数p-norm规范化)
	Normalizer是一个转换器，它可以将一组特征向量（通过计算p-范数）规范化。参数为p（默认值：2）来指定规范化中使用的p-norm。规范化操作可以使输入数据标准化，对后期机器学习算法的结果也有更好的表现。
	val normalizer = new Normalizer().setP(1.0) -- L1正则化，默认L2
	两种正则化方法：
		1.（正则化特征）
			normalizer.transform(dataFrame)， -- 个人理解为全局性质的转换
			normalizer.transform(dataFrame,  normalizer.p -> 2) -- 重新指定为L2正则化
		  对数据集单个特征进行转换，减去平均值（特征对齐）或是标准的正则转换（使得该特征的平均值和标准差分别为0和1）
		2.（正则化特征向量）
			normalizer.transform(dataFrame, normalizer.p -> Double.PositiveInfinity) -- L^\infty正则化 - 对每一行除以每行的最大值，使其达到归一化的效果。

	L1(Lasso) 和 L2(Ridge) 的区别：
		1.使用L2可以得到平滑的权值(抛物线) -- L2范数是指向量各元素的平方和然后求平方根。在回归里面，有人把有它的回归叫“岭回归”（Ridge Regression），也有叫“权值衰减weight decay”。它的强大功效是改善机器学习里面一个非常重要的问题（过拟合fitting）。
		2.使用L1可以得到稀疏的权值(V线) -- 使用L1正则后的权值更新规则多了一项 η * λ * sgn(w)/n，这一项 当w为正时，更新后的w变小。当w为负时，更新后的w变大——因此它的效果就是让w往0靠，使网络中的权重尽可能为0，也就相当于减小了网络复杂度，防止过拟合。所以说L1可以得到更稀疏的权值。

	我们让L2范数的规则项||W||2最小，可以使得W的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0，这里是有很大的区别的哦--平滑与稀疏。
	从优化或者数值计算的角度来说，L2范数有助于处理 condition number（局部最小值太多，那我们的优化算法就很容易陷入局部最小而不能自拔）不好的情况下矩阵求逆很困难的问题。因为目标函数如果是二次的，对于线性回归来说，那实际上是有解析解的，求导并令导数等于零即可得到最优解
	总结：L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。Lasso在特征选择时候非常有用，而Ridge就只是一种规则化而已。


2.StandardScaler（z-score规范化：零均值标准化）
	可以将输入的一组Vector特征向量规范化（标准化），使其有统一的的标准差（均方差）以及均值为0。它需要如下参数：
	1. withStd：默认值为真，将数据缩放到统一标准差方式。
	2. withMean：该方法将产出一个稠密的输出向量，所以--（不适用于（稀疏）向量。对于稀疏输入矩阵-需设为false。

	StandardScaler是一个Estimator，它可以通过拟合（fit）数据集产生一个StandardScalerModel，用来统计汇总。
		scalerModel.std -- 求列方差
		scalerModel.mean -- 求列均值

	new StandardScaler().setWithStd(true).setWithMean(true)--（所有特征减去列均值）(除以标准差->方差的平方根)-缩放

	注意：如果特征的标准差为零，则该特征在向量中返回的默认值为0.0。
	注意：尤其是（离群点）左右了MinMaxScaler规范化,需要使用StandardScaler。 
		MaxAbsScaler(绝对值规范化)不会破坏数据的稀疏性。


3.MinMaxScaler（最大-最小规范化） 
	MinMaxScaler将所有特征向量线性变换到指定范围（最小-最大值）之间（归一化到[min, max]，通常为[0,1]）。它的参数有：
	1. min：默认为0.0，为转换后所有特征的下边界。
	2. max：默认为1.0，为转换后所有特征的上边界。
	在计算时，该模型将特征向量一个个分开计算并转换到指定的范围内的。
	注意：
	（1）最大最小值可能受到离群值的左右。Bucketizer（去除离群值）
	（2）零值可能会转换成一个非零值，因此稀疏矩阵将变成一个稠密矩阵。


4.MaxAbsScaler(绝对值规范化)
	各特征值除以每个特征的最大值的绝对值，缩放到[-1,1]之间。因为它不会转移／集中数据，所以不会破坏数据的稀疏性。
	例如：特征向量[-1000,100,10],最大绝对值为1000，因此转换为[-1000/1000,100/100,10/1000]=[-1,0.1,0.01]。 
	注意：如果最大绝对值是一个离群点，显然这种处理方式是很不合理的。


5.Bucketizer
	分箱（分段处理）将（连续数值）转换为离散类别。-- 应用（去除离群值）
	splits（分箱数）：分箱数为n+1时，将产生n个区间。除了最后一个区间外，每个区间范围［x,y］由分箱的x，y决定。分箱必须是严格递增的。分箱（区间）见在分箱（区间）指定外的值将被归为错误。两个分裂的例子为Array(Double.NegativeInfinity, 0.0, 1.0, Double.PositiveInfinity)以及Array(0.0, 1.0, 2.0)。
	注意：
	   当不确定分裂的上下边界时，应当添加Double.NegativeInfinity和Double.PositiveInfinity以免越界。
	   分箱区间必须严格递增，例如： s0 < s1 < s2 < ... < sn

	// Double.NegativeInfinity：负无穷
	// Double.PositiveInfinity：正无穷
	// 分为6个组：[负无穷,-100),[-100,-10),[-10,0),[0,10),[10,90),[90,正无穷)

	val splits = Array(Double.NegativeInfinity, -100, -10, 0.0, 10, 90, Double.PositiveInfinity)
	val data: Array[Double] = Array(-180,-160,-100,-50,-70,-20,-8,-5,-3, 0.0, 1,3,7,10,30,60,90,100,120,150)

	val dataFrame = spark.createDataFrame(data.map(Tuple1.apply)).toDF("features")

	val bucketizer = new Bucketizer().setInputCol("features").setOutputCol("bucketedFeatures").setSplits(splits)
	val bucketedData = bucketizer.transform(dataFrame)

	bucketedData.show(50,truncate=false)
		+--------+----------------+
		|features|bucketedFeatures|
		+--------+----------------+
		|-180.0  |0.0             |
		|-160.0  |0.0             |
		|-100.0  |1.0             |
		|-50.0   |1.0             |
		|-70.0   |1.0             |
		|-20.0   |1.0             |
		|-8.0    |2.0             |
		|-5.0    |2.0             |
		|-3.0    |2.0             |
		|0.0     |3.0             |
		|1.0     |3.0             |
		|3.0     |3.0             |
		|7.0     |3.0             |
		|10.0    |4.0             |
		|30.0    |4.0             |
		|60.0    |4.0             |
		|90.0    |5.0             |
		|100.0   |5.0             |
		|120.0   |5.0             |
		|150.0   |5.0             |
		+--------+----------------+

   	应用：过滤离群值
		bucketedData.filter($"bucketedFeatures" <= 1).show(50,truncate=false)
		+--------+----------------+
		|features|bucketedFeatures|
		+--------+----------------+
		|-180.0  |0.0             |
		|-160.0  |0.0             |
		|-100.0  |1.0             |
		|-50.0   |1.0             |
		|-70.0   |1.0             |
		|-20.0   |1.0             |
		+--------+----------------+

	比如特征是年龄，是一个连续数值，需要将其转换为离散类别(未成年人、青年人、中年人、老年人），就要用到Bucketizer了。 


6.ElementwiseProduct (Hadamard乘积)
	（以列为单位），对输入向量的每个元素乘以一个权重（weight），即对输入向量每个元素逐个进行放缩。对输入向量v 和变换向量scalingVec 使用Hadamard product(阿达玛积)进行变换，最终产生一个新的向量。
	例如：
		val dataFrame = spark.createDataFrame(Seq(("a", Vectors.dense(1.0, 2.0, 3.0)),..)).toDF("id", "vector")
    		val transformingVector = Vectors.dense(0.0, 1.0, 2.0)
		val transformer = new ElementwiseProduct().setScalingVec(transformingVector)


7.SQLTransformer（SQL变换）
	用来转换由SQL定义的陈述。目前仅支持SQL语法如"SELECT ... FROM __THIS__ ..."，其中"__THIS__"代表输入数据的基础表(DataFrame)。
	例如：
		val sqlTrans = new SQLTransformer().setStatement(
      			"SELECT *, (v1 + v2) AS v3, (v1 * v2) AS v4 FROM __THIS__")
    		sqlTrans.transform(df).show()


8.VectorAssembler（特征向量合并）
	VectorAssembler是一个转换器，它将给定的(若干列)合并为(单列)向量。它可以将原始特征和一系列通过其他转换器变换得到的特征合并为单一的特征向量，来训练如逻辑回归和决策树等机器学习算法。VectorAssembler可接受的输入列类型：数值型、布尔型、向量型。输入列的值将按指定顺序依次添加到一个新向量中。
	例如：
		new VectorAssembler().setInputCols(Array("hour", "mobile", "userFeatures")).setOutputCol("features")
		.transform(dataset)

	原始DF：
		id | hour | mobile | userFeatures     | clicked
		----|------|--------|------------------|---------
		 0  | 18   | 1.0    | [0.0, 10.0, 0.5] | 1.0

	合并后：
		id | hour | mobile | userFeatures     | clicked | features
		----|------|--------|------------------|---------|-----------------------------
		 0  | 18   | 1.0    | [0.0, 10.0, 0.5] | 1.0     | [18.0, 1.0, 0.0, 10.0, 0.5]


9.QuantileDiscretizer（分位数离散化）
	 和Bucketizer（分箱处理）一样：将连续数值特征转换为离散类别特征。实际上Class QuantileDiscretizer extends （继承自） Class（Bucketizer）。不同的是这里不再自己定义splits（分类标准），而是定义分几箱(段）就可以。QD自己调用函数计算分位数，并完成离散化。 
	
	-参数1：（分级的数量）由numBuckets参数决定。
	-参数3：（分级的范围）由渐进算法（approxQuantile ）决定。上下边界将设置为正（+Infinity）负（-Infinity）无穷，覆盖所有实数范围。
	-参数2：（渐进的精度）由relativeError参数决定。设置为0时，将会计算精确的分位点（计算代价较高）。

	例如：
		val discretizer = new QuantileDiscretizer().setInputCol("hour").setOutputCol("result")
			.setNumBuckets(3) //分3（桶/段/箱）
			.setRelativeError(0.1) //设置precision-控制相对误差
		val result = discretizer.fit(df).transform(df)




三/ Feature Selectors（特征选择）/-------------------------------------------------------------------/

1.VectorSlicer（向量选择）
	将输入特征向量的值进行筛选得到新的向量集-输出原始特征向量子集的转换器。VectorSlicer对特征提取非常有帮助。
	VectorSlicer接收带有特定索引的向量列，如下两种索引: 
		1. 整数索引，setIndices()
		2. 字符串索引，setNames()，此类要求向量列有AttributeGroup，因为该工具根据Attribute来匹配属性字段。
	索引可以指定整数或者字符串类型。另外，也可以同时使用整数索引和字符串名字。不允许使用重复的特征，所以所选的索引或者名字必须是独一的。注意如果使用名字特征，当遇到空值的时候将会抛异常。
	输出将会首先按照所选的数字索引排序（按输入顺序），其次按名字排序（按输入顺序）。

    import java.util.Arrays

    import org.apache.spark.ml.attribute.{Attribute, AttributeGroup, NumericAttribute}
    import org.apache.spark.ml.feature.VectorSlicer
    import org.apache.spark.ml.linalg.Vectors
    import org.apache.spark.sql.Row
    import org.apache.spark.sql.types.StructType

    val data = Arrays.asList(
      Row(org.apache.spark.ml.linalg.Vectors.sparse(3, Seq((0, -2.0), (1, 2.3)))),
      Row(org.apache.spark.ml.linalg.Vectors.dense(-2.0, 2.3, 0.0))
    )

    val defaultAttr = NumericAttribute.defaultAttr
    val attrs = Array("f1", "f2", "f3").map(defaultAttr.withName)
    val attrGroup = new AttributeGroup("userFeatures", attrs.asInstanceOf[Array[Attribute]])

    val dataset = spark.createDataFrame(data, StructType(Array(attrGroup.toStructField())))

    val slicer = new VectorSlicer().setInputCol("userFeatures").setOutputCol("features")

    slicer.setIndices(Array(1)).setNames(Array("f3")) //两种方法，取第1列和"f3"列
    // slicer.setIndices(Array(1, 2))
    // or slicer.setNames(Array("f2", "f3")),取这两列

    val output = slicer.transform(dataset)
    output.show(false)


2.RFormula（R模型公式） 
	RFormula产生一个特征向量和一个double或者字符串标签列（label）。就如R中使用formulas一样，字符型的输入将转换成one-hot编码，数字输入转换成双精度。如果类别列是字符串类型，它将通过StringIndexer转换为double类型索引。如果标签列不存在，则formulas输出中将通过特定的响应变量创造一个标签列。 
	例如：使用RFormula公式clicked ~ country + hour，则表明我们希望基于country 和hour预测clicked，

   import org.apache.spark.ml.feature.RFormula

    val dataset = spark.createDataFrame(Seq(
      (7, "US", 18, 1.0),
      (8, "CA", 12, 0.0),
      (9, "NZ", 15, 0.0)
    )).toDF("id", "country", "hour", "clicked")
    val formula = new RFormula().setFormula("clicked ~ country + hour").setFeaturesCol("features").setLabelCol("label")
    val output = formula.fit(dataset).transform(dataset)
    output.select("features", "label").show()


3.ChiSqSelector（卡方特征选择）
	从矩阵中取出/简化/最有用的列，如类别特征 -- 它适用于(带有)类别特征的(标签)数据。ChiSqSelector根据分类的卡方独立性检验来对特征排序，然后选取类别标签最主要依赖的特征。它类似于选取最有预测能力的特征。
Examples：
	假设我们有一个DataFrame含有id, features和clicked三列，其中clicked为需要预测的目标：
	id | features              | clicked
	---|-----------------------|---------
	 7 | [0.0, 0.0, 18.0, 1.0] | 1.0
	 8 | [0.0, 1.0, 12.0, 0.0] | 0.0
	 9 | [1.0, 0.0, 15.0, 0.1] | 0.0
	如果我们使用ChiSqSelector并设置numTopFeatures为1，根据标签clicked，features中最后一列将会是最有用特征：
	 
	id | features              | clicked | selectedFeatures
	---|-----------------------|---------|------------------
	 7 | [0.0, 0.0, 18.0, 1.0] | 1.0     | [1.0]
	 8 | [0.0, 1.0, 12.0, 0.0] | 0.0     | [0.0]
	 9 | [1.0, 0.0, 15.0, 0.1] | 0.0     | [0.1]

	import org.apache.spark.ml.feature.ChiSqSelector
	import org.apache.spark.ml.linalg.Vectors

	val data = Seq(
	  (7, Vectors.dense(0.0, 0.0, 18.0, 1.0), 1.0),
	  (8, Vectors.dense(0.0, 1.0, 12.0, 0.0), 0.0),
	  (9, Vectors.dense(1.0, 0.0, 15.0, 0.1), 0.0)
	)

	val df = spark.createDataset(data).toDF("id", "features", "clicked")

	val selector = new ChiSqSelector()
	  .setNumTopFeatures(1)
	  .setFeaturesCol("features")
	  .setLabelCol("clicked")
	  .setOutputCol("selectedFeatures")

	val result = selector.fit(df).transform(df)

	println(s"ChiSqSelector output with top ${selector.getNumTopFeatures} features selected")
	result.show()


4.Locality Sensitive Hashing（局部敏感哈希）
	局部敏感哈希（LSH）是散列技术中重要的一类，常用的聚类方法，近似最近邻搜索和大型数据集的离群点检测。
	1.使用LSH进行对海量数据建立索引，设置指定的哈希表的数量setNumHashTables()。output类型是[向量]数组，维数等于numhashtables。
	2.设置setBucketLength(2.0)桶的数量（类似于类别）


    import org.apache.spark.ml.feature.BucketedRandomProjectionLSH
    import org.apache.spark.ml.linalg.Vectors

    val dfA = spark.createDataFrame(Seq(
      (0, Vectors.dense(1.0, 1.0)),
      (1, Vectors.dense(1.0, -1.0)),
      (2, Vectors.dense(-1.0, -1.0)),
      (3, Vectors.dense(-1.0, 1.0))
    )).toDF("id", "keys")

    val dfB = spark.createDataFrame(Seq(
      (4, Vectors.dense(1.0, 0.0)),
      (5, Vectors.dense(-1.0, 0.0)),
      (6, Vectors.dense(0.0, 1.0)),
      (7, Vectors.dense(0.0, -1.0))
    )).toDF("id", "keys")

    val key = Vectors.dense(1.0, 0.0)

    val brp = new BucketedRandomProjectionLSH()
      .setBucketLength(2.0)
      .setNumHashTables(3)
      .setInputCol("keys")
      .setOutputCol("values")

    val model = brp.fit(dfA)

    // Feature Transformation
    model.transform(dfA).show()
    // Cache the transformed columns
    val transformedA = model.transform(dfA).cache()
    val transformedB = model.transform(dfB).cache()

    // Approximate similarity join 相似度， approx（大约 大概）
    model.approxSimilarityJoin(dfA, dfB, 1.5).show()
    model.approxSimilarityJoin(transformedA, transformedB, 1.5).show()
    // Self Join
    model.approxSimilarityJoin(dfA, dfA, 2.5).filter("datasetA.id < datasetB.id").show()

    // Approximate nearest neighbor search 最近邻
    model.approxNearestNeighbors(dfA, key, 2).show()
    model.approxNearestNeighbors(transformedA, key, 2).show()


LSH的应用：

LSH的应用场景很多，凡是需要进行大量数据之间的相似度（或距离）计算的地方都可以使用LSH来加快查找匹配速度，下面列举一些应用：
（1）查找网络上的重复网页
互联网上由于各式各样的原因（例如转载、抄袭等）会存在很多重复的网页，因此为了提高搜索引擎的检索质量或避免重复建立索引，需要查找出重复的网页，以便进行一些处理。其大致的过程如下：将互联网的文档用一个集合或词袋向量来表征，然后通过一些hash运算来判断两篇文档之间的相似度，常用的有minhash+LSH、simhash。
（2）查找相似新闻网页或文章
与查找重复网页类似，可以通过hash的方法来判断两篇新闻网页或文章是否相似，只不过在表达新闻网页或文章时利用了它们的特点来建立表征该文档的集合。
（3）图像检索
（4）音乐检索
（5）指纹匹配

具体原理：http://blog.csdn.net/icvpr/article/details/12342159




将向量转成DF，先转成Tuple。
    val data = spark.createDataFrame(data1.map(Tuple1.apply)).toDF("features")


