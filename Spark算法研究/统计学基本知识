

预测误差有两个主要原因:偏差和方差。

偏差：
    偏差是对预测结果与实际值的估计差距。又称为表观误差，是指个别测定值与测定的平均值之差，它可以用来衡量测定结果的精密度高低

标准差(Standard Deviation)：
    各数据偏离平均数的距离(离均差)的平均数,它是离差平方和平均后的方根。用σ表示。因此,标准差也是一种平均数

方差(Variance)：
    方差是估计不同预测中预测值的差异。和标准差是测度数据变异程度的最重要、最常用的指标。衡量随机变量或一组数据时离散程度的度量。概率论中方差用来度量随机变量和其数学期望（即均值）之间的偏离程度。

    优化： 如果你的模型有很高的方差，您可以删除特性以减少它。更大的数据集也有助于减少差异。

残差：
    真实值和预测值之间的差值，“真实值-预测值”得到残差。GBDTs惨差版其核心思想是每轮通过拟合残差来降低损失函数。


系数矩阵：
      mlrModel.
        coefficientMatrix       // 计算（系数矩阵）,线性代数里面的概念,就是非齐次线性方程组(常数项不全为零的线性方程组)的系数组成的那个矩阵.
        // 系数矩阵是矩阵中的众多类型之一，简单来说系数矩阵就是将方程组的系数组成矩阵来计算方程的解。系数矩阵常常用来表示一些项目的数学关系，比如通过此类关系系数矩阵来证明各项目的正反比关系。

      提高预测质量的2种方法：
          在lasso，很多的功能得到它们的系数设置为零，因此，从等式消除这些。
          在ridge， predictors or features are penalized，但从来没有设置为零,也接近于0.


  例子1（linear	regression	with	lasso -- L1）：
  ：
      scala>	val	points	=	spark.createDataFrame(Seq(
      				      (1d,Vectors.dense(5,3,1,2,1,3,2,2,1)),
      				      (2d,Vectors.dense(9,8,8,9,7,9,8,7,9))
              )).toDF("label","features")
              // in range [0, 1]. For alpha = 1, it is an L1 penalty
      scala>	val	lr	=	new	LinearRegression().setMaxIter(10).setRegParam(.3).setFitIntercept(false).setElasticNetPar
      scala>	val	model	=	lr.fit(points)

      检查有多少个预测因子的系数被设为零:
        scala>	model.coefficients
        org.apache.spark.ml.linalg.Vector	=	[0.17372423795367625,0.025461270520367753,0.0,0.0,0.0,0.015585497064731
      正如你所看到的，9个预测因子中有6个将它们的系数设为零。
      这是主要的lasso的特性:任何它认为没有用的预测器，都设置他们的系数为零来从等式中移出这些无用的特征。


  例子2（ridge	regression  -- L2）：
  ：
        scala>	val	points	=	spark.createDataFrame(Seq(
                  (1d,Vectors.dense(5,3,1,2,1,3,2,2,1)),
                  (2d,Vectors.dense(9,8,8,9,7,9,8,7,9))
                )).toDF("label","features")
                // in range [0, 1]. For alpha = 0, the penalty is an L2 penalty.
        scala>	val	lr	=	new	LinearRegression().setMaxIter(10).setRegParam(.3).setFitIntercept(false).setElasticNetPar
        scala>	val	model	=	lr.fit(points)
        scala>	model.coefficients
            		org.apache.spark.ml.linalg.Vector	=	[0.1132933163345012,0.039370733000466666,0.002369276442275222,
            		0.01041698759881142,0.004328988574203182,0.026236646722551202,
            		0.015282817648377045,0.023597219133656675,0.0011928984792447484]

        正如你所看到的，不像L1，岭回归不分配任何预测系数的值为零，但它确实分配一些非常接近于零。





监督机器学习问题无非就是在规则化参数的同时最小化误差。
    最小化误差是为了让我们的模型拟合我们的训练数据，而规则化参数是防止我们的模型过分拟合我们的训练数据。


logLikelihood：
    likelihood可以理解为"probability"。对数似然估计函数 值一般取负值，实际值（不是绝对值）越大越好。<-实际值越大越好！！
    第一，基本推理。对于似然函数，如果是离散分布，最后得到的数值直接就是概率，取值区间为0-1，对数化之后的值就是负数了；
        如果是连续变量，因为概率密度函数的取值区间并不局限于0-1，所以最后得到的似然函数值不是概率而只是概率密度函数值，这样对数化之后的正负就不确定了。
    第二，Eviews的计算公式解释。公式值的大小关键取之于残差平方和（以及样本容量），只有当残差平方和与样本容量的比值很小时，括号内的值才可能为负，从而公式值为正，这时说明参数拟合效度很高；反之公式值为负，但其绝对值越小表示残差平方和越小，因而参数拟合效度越高。

    似然函数值的自然对数的—2倍，常用来反映模型的拟合程度，其值越小，表示拟合程度越好.
    似然比统计量，服从卡方分布，是一个检验统计量，无所谓越大越好还是越小越好。该统计量大于卡方临界值时，拒绝原假设，否则接受原假设

    正负都有可能，主要取决于随机误差项方差的估计值的大小，如果该值较小，则为正，反之，则为负。Log likehood就是对数似然函数的值，


learningRate（学习率）-- 梯度提升树 and LDA中涉及
    这个参数一般不需要调试，如果发现算法面对某个数据集，变现得极其不稳定，那么就要减小学习率再试一下，一般会有改善（稳定性变好）。小的学习率（步长）肯定会增加训练的时间。

    2.学习率（步长）：学习率也会影响预测准确率，设置太大精度会降低。
       设置一个非常小的学习率=0.05，逐步增加弱分类器的数目 ，可以看出学习率很小时，的确需要很多的弱分类器才能得到较好的结果。但是预测效果一直在变好。
       学习率很大时，较少的n_estimators 值就可以达到类似的结果。（但是考虑到模型的稳定，还是不建议选一个很大的学习率）



计算相关性（Calculating	correlation）
    相关性是两个变量之间的统计关系，当一个变量发生变化时，导致另一个变量的变化。
      相关分析衡量的是两者的程度变量是相关的。
    我们在日常生活中看到了关联。一个人的身高与一个人的体重有关，卡车承载能力与车轮的数量有关，等等。
      如果一个变量的增加导致另一个变量的增加，它被称为正相关。如果一个变量的增加导致另一个变量的减少，这是一个负相关。

    spark支持两种关联算法:皮尔森和斯皮尔曼（Pearson	and	Spearman.）。
        ：皮尔逊算法 适用两个连续变量，如一个人的身高、体重或房子的大小和房价。
        ：斯皮尔曼 处理一个连续的和一个分类变量，例如，邮政编码和房价。


       2.	 Create	a	DataFrame	of	house	price	and	size:
       								scala>		val	houses	=	spark.createDataFrame(Seq(
       								(1620000d,2100),
       								(1690000d,2300),
       								(1400000d,2046),
       								(2000000d,4314),
       								(1060000d,1244),
       								(3830000d,4608),
       								(1230000d,2173),
       								(2400000d,2750),
       								(3380000d,4010),
       								(1480000d,1959)
       								)).toDF("price","size")
       3.	 Compute	the	correlation:
       								scala>	houses.stat.corr("price","size")
       								correlation:	Double	=	0.8577177736252574

                      (结果表现出了非常强的正相关性)
