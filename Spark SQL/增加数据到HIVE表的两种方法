

如果是外部数据，可以用external外部表，每天用hadoop fs -put的方法将数据复制到表目录中即可，这样也可以用于除了Hive的其他程序读取；

如果是中间表、临时表、产出表，则可以使用内部表，每天计算全量覆盖这些表内容；



















